{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 0,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Concise Implementation of Linear Regression\n",
    "\n",
    "\n",
    "* implement the linear regression model concisely by using high-level APIs of deep learning frameworks.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 0,
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "*NOTE:*\n",
    "\n",
    "Broad and intense interest in deep learning for the past several years\n",
    "has inspired companies, academics, and hobbyists\n",
    "to develop a variety of mature open source frameworks\n",
    "for automating the repetitive work of implementing\n",
    "gradient-based learning algorithms.\n",
    "In `sec_linear_scratch`, we relied only on\n",
    "(i) tensors for data storage and linear algebra;\n",
    "and (ii) auto differentiation for calculating gradients.\n",
    "In practice, because data iterators, loss functions, optimizers,\n",
    "and neural network layers\n",
    "are so common, modern libraries implement these components for us as well.\n",
    "\n",
    "In this section, we will show you how to implement\n",
    "the linear regression model from `sec_linear_scratch`\n",
    "concisely by using high-level APIs of deep learning frameworks.\n",
    "\n",
    "\n",
    "## Generating the Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "origin_pos": 2,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "from d2l import torch as d2l\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils import data\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "\n",
    "* Start generating the dataset of linear data \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "origin_pos": 4,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features: tensor([-1.1053,  0.1650]) \n",
      "label: tensor([-0.9287])\n"
     ]
    }
   ],
   "source": [
    "true_w = torch.tensor([1.0, 1.0])\n",
    "true_b = 0\n",
    "features, labels = d2l.synthetic_data(true_w, true_b, 1000)\n",
    "print('features:', features[0],'\\nlabel:', labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(features[:, 1], labels, 1);\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Label');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 5,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Reading the Dataset\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 5,
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "*NOTE:*\n",
    "\n",
    "Rather than rolling our own iterator,\n",
    "we can call upon the existing API in a framework to read data.\n",
    "We pass in `features` and `labels` as arguments and specify `batch_size`\n",
    "when instantiating a data iterator object.\n",
    "Besides, the boolean value `is_train`\n",
    "indicates whether or not\n",
    "we want the data iterator object to shuffle the data\n",
    "on each epoch (pass through the dataset).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "code_folding": [],
    "origin_pos": 7,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "def load_array(data_arrays, batch_size, is_train=True):  #@save\n",
    "    \"\"\"Construct a PyTorch data iterator.\"\"\"\n",
    "    dataset = data.TensorDataset(*data_arrays)\n",
    "    return data.DataLoader(dataset, batch_size, shuffle=is_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "origin_pos": 9,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data_iter has 100 minibatches of 10 elements each\n"
     ]
    }
   ],
   "source": [
    "batch_size = 10\n",
    "data_iter = load_array((features, labels), batch_size)\n",
    "print(f\"Data_iter has {len(data_iter)} minibatches of {batch_size} elements each\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 10,
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "*NOTE:*\n",
    "\n",
    "Now we can use `data_iter` in much the same way as we called\n",
    "the `data_iter` function in `sec_linear_scratch`.\n",
    "To verify that it is working, we can read and print\n",
    "the first minibatch of examples.\n",
    "Comparing with `sec_linear_scratch`,\n",
    "here we use `iter` to construct a Python iterator and use `next` to obtain the first item from the iterator.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "origin_pos": 11,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[ 0.9731,  0.8248],\n",
       "         [-1.0111, -0.4135],\n",
       "         [-0.9055,  0.2161],\n",
       "         [-1.1184,  0.3672],\n",
       "         [-1.9046,  0.8282],\n",
       "         [ 1.0690,  0.0706],\n",
       "         [-1.3450, -2.5737],\n",
       "         [ 1.5622,  0.2870],\n",
       "         [-1.7053,  0.4414],\n",
       "         [ 0.1675,  1.7562]]),\n",
       " tensor([[ 1.8022],\n",
       "         [-1.4206],\n",
       "         [-0.6928],\n",
       "         [-0.7548],\n",
       "         [-1.0749],\n",
       "         [ 1.1378],\n",
       "         [-3.9074],\n",
       "         [ 1.8545],\n",
       "         [-1.2719],\n",
       "         [ 1.9333]])]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(data_iter))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 12,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Defining the Model\n",
    "\n",
    "*  a model is a sequence of layer. \n",
    "    * `Sequential` is a class that defines a container for several layers, chained together\n",
    "    * `Linear` is used in  pyTorch to define a fully-connected layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 12,
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "*NOTE:*\n",
    "\n",
    "When we implemented linear regression from scratch\n",
    "in `sec_linear_scratch`,\n",
    "we defined our model parameters explicitly\n",
    "and coded up the calculations to produce output\n",
    "using basic linear algebra operations.\n",
    "You *should* know how to do this.\n",
    "But once your models get more complex,\n",
    "and once you have to do this nearly every day,\n",
    "you will be glad for the assistance.\n",
    "The situation is similar to coding up your own blog from scratch.\n",
    "Doing it once or twice is rewarding and instructive,\n",
    "but you would be a lousy web developer\n",
    "if every time you needed a blog you spent a month\n",
    "reinventing the wheel.\n",
    "\n",
    "For standard operations, we can use a framework's predefined layers,\n",
    "which allow us to focus especially\n",
    "on the layers used to construct the model\n",
    "rather than having to focus on the implementation.\n",
    "We will first define a model variable `net`,\n",
    "which will refer to an instance of the `Sequential` class.\n",
    "The `Sequential` class defines a container\n",
    "for several layers that will be chained together.\n",
    "Given input data, a `Sequential` instance passes it through\n",
    "the first layer, in turn passing the output\n",
    "as the second layer's input and so forth.\n",
    "In the following example, our model consists of only one layer,\n",
    "so we do not really need `Sequential`.\n",
    "But since nearly all of our future models\n",
    "will involve multiple layers,\n",
    "we will use it anyway just to familiarize you\n",
    "with the most standard workflow.\n",
    "\n",
    "Recall the architecture of a single-layer network as shown in `fig_single_neuron`.\n",
    "The layer is said to be *fully-connected*\n",
    "because each of its inputs is connected to each of its outputs\n",
    "by means of a matrix-vector multiplication.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 14,
    "slideshow": {
     "slide_type": "notes"
    },
    "tab": [
     "pytorch"
    ]
   },
   "source": [
    "In PyTorch, the fully-connected layer is defined in the `Linear` class. Note that we passed two arguments into `nn.Linear`. The first one specifies the input feature dimension, which is 2, and the second one is the output feature dimension, which is a single scalar and therefore 1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "origin_pos": 17,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "# `nn` is an abbreviation for neural networks\n",
    "from torch import nn\n",
    "net = nn.Sequential(nn.Linear(2, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 19,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Initializing Model Parameters\n",
    "\n",
    "* `net[0]` locate the layer of interest (in this example only one layer!)\n",
    "* `weight.data` and `bias.data` methods allow to access the parameters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 19,
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "*NOTE:*\n",
    "\n",
    "Before using `net`, we need to initialize the model parameters,\n",
    "such as the weights and bias in the linear regression model.\n",
    "Deep learning frameworks often have a predefined way to initialize the parameters.\n",
    "Here we specify that each weight parameter\n",
    "should be randomly sampled from a normal distribution\n",
    "with mean 0 and standard deviation 0.01.\n",
    "The bias parameter will be initialized to zero.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 21,
    "slideshow": {
     "slide_type": "notes"
    },
    "tab": [
     "pytorch"
    ]
   },
   "source": [
    "As we have specified the input and output dimensions when constructing `nn.Linear`. Now we access the parameters directly to specify their initial values. We first locate the layer by `net[0]`, which is the first layer in the network, and then use the `weight.data` and `bias.data` methods to access the parameters. Next we use the replace methods `normal_` and `fill_` to overwrite parameter values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "origin_pos": 24,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0083, -0.0061]])\n",
      "tensor([0.])\n"
     ]
    }
   ],
   "source": [
    "print(net[0].weight.data.normal_(0, 0.01))\n",
    "print(net[0].bias.data.fill_(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 29,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Defining the Loss Function\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 31,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tab": [
     "pytorch"
    ]
   },
   "source": [
    "The `MSELoss` class computes the mean squared error, also known as squared $L_2$ norm.\n",
    "By default it returns the average loss over examples.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "origin_pos": 34,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "loss = nn.MSELoss() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 36,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Defining the Optimization Algorithm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 38,
    "slideshow": {
     "slide_type": "notes"
    },
    "tab": [
     "pytorch"
    ]
   },
   "source": [
    "*NOTES:*\n",
    "\n",
    "Minibatch stochastic gradient descent is a standard tool\n",
    "for optimizing neural networks\n",
    "and thus PyTorch supports it alongside a number of\n",
    "variations on this algorithm in the `optim` module.\n",
    "When we instantiate an `SGD` instance,\n",
    "we will specify the parameters to optimize over\n",
    "(obtainable from our net via `net.parameters()`), with a dictionary of hyperparameters\n",
    "required by our optimization algorithm.\n",
    "Minibatch stochastic gradient descent just requires that\n",
    "we set the value `lr`, which is set to 0.03 here.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "origin_pos": 41,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "trainer = torch.optim.SGD(net.parameters(), lr=0.03)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automatic Differentiation\n",
    "\n",
    "\n",
    "* Differentiation is a crucial step in nearly all deep learning optimization algorithms.\n",
    "\n",
    "* *automatic differentiation*: the system build a *computational graph*, tracking which data combined through which operations to produce the output.\n",
    "* Automatic differentiation enables the system to subsequently backpropagate gradients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "As we have explained \n",
    "differentiation is a crucial step in nearly all deep learning optimization algorithms.\n",
    "While the calculations for taking these derivatives are straightforward,\n",
    "requiring only some basic calculus,\n",
    "for complex models, working out the updates by hand\n",
    "can be a pain (and often error-prone).\n",
    "\n",
    "Deep learning frameworks expedite this work\n",
    "by automatically calculating derivatives, i.e., *automatic differentiation*.\n",
    "In practice,\n",
    "based on our designed model\n",
    "the system builds a *computational graph*,\n",
    "tracking which data combined through\n",
    "which operations to produce the output.\n",
    "Automatic differentiation enables the system to subsequently backpropagate gradients.\n",
    "Here, *backpropagate* simply means to trace through the computational graph,\n",
    "filling in the partial derivatives with respect to each parameter.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gradient w.r.t x:\n",
    "\n",
    "a = torch.tensor(2)\n",
    "b = torch.tensor(3)\n",
    "x = torch.tensor(4.0,  requires_grad=True)\n",
    "y = a*x\n",
    "z = y**2 + b\n",
    "z.backward()   # call backward computation\n",
    "x.grad         # access the gradient w.r.t. x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#OR Gradient w.r.t y:\n",
    "a = torch.tensor(2)\n",
    "b = torch.tensor(3)\n",
    "x = torch.tensor(4.0)\n",
    "y = a*x\n",
    "y.requires_grad=True\n",
    "z = y**2 + b\n",
    "z.backward()   # call backward computation\n",
    "y.grad  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 43,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Training\n",
    "\n",
    "* For some number of epochs:\n",
    "    * make a complete pass over the dataset (`train_data`), iteratively grabbing one minibatch of inputs and labels.\n",
    "    * For each minibatch \n",
    "        * Generate predictions by calling `net(X)` and calculate the loss `l` (the forward propagation).\n",
    "        * Calculate gradients by running the backpropagation.\n",
    "        * Update the model parameters by invoking our optimizer.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 43,
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "*NOTES:*\n",
    "\n",
    "You might have noticed that expressing our model through\n",
    "high-level APIs of a deep learning framework\n",
    "requires comparatively few lines of code.\n",
    "We did not have to individually allocate parameters,\n",
    "define our loss function, or implement minibatch stochastic gradient descent.\n",
    "Once we start working with much more complex models,\n",
    "advantages of high-level APIs will grow considerably.\n",
    "However, once we have all the basic pieces in place,\n",
    "the training loop itself is strikingly similar\n",
    "to what we did when implementing everything from scratch.\n",
    "\n",
    "To refresh your memory: for some number of epochs,\n",
    "we will make a complete pass over the dataset (`train_data`),\n",
    "iteratively grabbing one minibatch of inputs\n",
    "and the corresponding ground-truth labels.\n",
    "For each minibatch, we go through the following ritual:\n",
    "\n",
    "* Generate predictions by calling `net(X)` and calculate the loss `l` (the forward propagation).\n",
    "* Calculate gradients by running the backpropagation.\n",
    "* Update the model parameters by invoking our optimizer.\n",
    "\n",
    "For good measure, we compute the loss after each epoch and print it to monitor progress.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "origin_pos": 45,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, loss 0.000115\n",
      "epoch 2, loss 0.000099\n",
      "epoch 3, loss 0.000099\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 3\n",
    "j=0\n",
    "for epoch in range(num_epochs):\n",
    "    for X, y in data_iter:   \n",
    "        l = loss(net(X) ,y) # Forward propagation\n",
    "        trainer.zero_grad() # To avoid accumulating the gradients on subsequent backward passes\n",
    "        l.backward()        # Compute the gradient cumulating over the minibatch\n",
    "        trainer.step()      # Update the weights\n",
    "    l = loss(net(features), labels) #evaluate the loww with the new weigths\n",
    "    print(f'epoch {epoch + 1}, loss {l:f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 47,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Comparison between learnt and actual parameters \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 47,
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Below, we compare the model parameters learned by training on finite data\n",
    "and the actual parameters that generated our dataset.\n",
    "To access parameters,\n",
    "we first access the layer that we need from `net`\n",
    "and then access that layer's weights and bias.\n",
    "As in our from-scratch implementation,\n",
    "note that our estimated parameters are\n",
    "close to their ground-truth counterparts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "origin_pos": 49,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error in estimating w: tensor([0.0012, 0.0001])\n",
      "error in estimating b: tensor([0.0003])\n"
     ]
    }
   ],
   "source": [
    "w = net[0].weight.data\n",
    "print('error in estimating w:', true_w - w.reshape(true_w.shape))\n",
    "b = net[0].bias.data\n",
    "print('error in estimating b:', true_b - b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 51,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Summary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 53,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tab": [
     "pytorch"
    ]
   },
   "source": [
    "* Using PyTorch's high-level APIs, we can implement models much more concisely.\n",
    "* In PyTorch, the `data` module provides tools for data processing, the `nn` module defines a large number of neural network layers and common loss functions.\n",
    "* We can initialize the parameters by replacing their values with methods ending with `_`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 55,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Putting All Things Together\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, loss 0.000192\n",
      "epoch 2, loss 0.000102\n",
      "epoch 3, loss 0.000103\n",
      "\n",
      "error in estimating w: tensor([4.9543e-04, 5.5313e-05])\n",
      "error in estimating b: tensor([-0.0012])\n"
     ]
    }
   ],
   "source": [
    "from d2l import torch as d2l\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils import data\n",
    "from torch import nn\n",
    "\n",
    "def load_array(data_arrays, batch_size, is_train=True):  #@save\n",
    "    \"\"\"Construct a PyTorch data iterator.\"\"\"\n",
    "    dataset = data.TensorDataset(*data_arrays)\n",
    "    return data.DataLoader(dataset, batch_size, shuffle=is_train)\n",
    "\n",
    "def init_weights(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        nn.init.normal_(m.weight, std=0.01)\n",
    "        \n",
    "# generate synthetic data:\n",
    "true_w = torch.tensor([2, -3.4])\n",
    "true_b = 4.2\n",
    "features, labels = d2l.synthetic_data(true_w, true_b, 1000)\n",
    "\n",
    "# organize data in batches\n",
    "batch_size = 10\n",
    "data_iter = load_array((features, labels), batch_size)\n",
    "\n",
    "# define the model and initialize its weights\n",
    "net = nn.Sequential(nn.Linear(2, 1))\n",
    "net.apply(init_weights)\n",
    "\n",
    "# Set a Loss function and the Optimization algorithm\n",
    "loss = nn.MSELoss() \n",
    "trainer = torch.optim.SGD(net.parameters(), lr=0.03)\n",
    "\n",
    "# Train the Model\n",
    "num_epochs = 3\n",
    "for epoch in range(num_epochs):\n",
    "    for X, y in data_iter:\n",
    "        l = loss(net(X) ,y)\n",
    "        trainer.zero_grad() \n",
    "        l.backward()\n",
    "        trainer.step()\n",
    "    l = loss(net(features), labels)\n",
    "    print(f'epoch {epoch + 1}, loss {l:f}')\n",
    "\n",
    "#Check the quality of the learnt parameters\n",
    "w = net[0].weight.data\n",
    "print('\\nerror in estimating w:', true_w - w.reshape(true_w.shape))\n",
    "b = net[0].bias.data\n",
    "print('error in estimating b:', true_b - b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 55,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Exercises\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 57,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tab": [
     "pytorch"
    ]
   },
   "source": [
    "1. How many times is it computed the following loop in the example above?\n",
    "\n",
    "`for X, y in data_iter:\n",
    "        l = loss(net(X) ,y)\n",
    "        trainer.zero_grad() \n",
    "        l.backward()\n",
    "        trainer.step()\n",
    "    l = loss(net(features), labels`\n",
    "\n",
    "    \n",
    "2. What happen at each loop execution? How could you augment/reduce the number of loops?\n",
    "3. Review the PyTorch documentation (https://pytorch.org/docs/stable/nn.html#loss-functions))to see what loss functions and initialization methods are provided. Replace the loss by L1 loss.\n",
    "4. How do you access the gradient of `net[0].weight`? (Do you remember what is net[0]?)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0028, -0.0047]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ex1\n",
    "# 100 times for each epoch (it corresponds to the Training set dimension/ Batch size: 1000/10)\n",
    "# at each loop execution the weights are updated\n",
    "# to augment, I should reduce the batch size and viceversa\n",
    "\n",
    "#Ex2\n",
    "\n",
    "loss = nn.L1Loss()\n",
    "\n",
    "#Ex3\n",
    "net[0].weight.grad"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
